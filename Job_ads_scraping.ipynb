{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping and NLP Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pprint\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pages to be scraped: object of the analysis\n",
    "Each page contains 25 job ads + 2 advertised ones. I have decided to include the advertised ones with the first page, since they repeat on the second page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://www.reed.co.uk/jobs/data-scientist-jobs-in-london?fulltime=True&proximity=20'\n",
    "page = requests.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup1 = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL2 = 'https://www.reed.co.uk/jobs/data-scientist-jobs-in-london?pageno=2&fulltime=True&proximity=20'\n",
    "page2 = requests.get(URL2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup2 = BeautifulSoup(page2.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL3 = 'https://www.reed.co.uk/jobs/data-scientist-jobs-in-london?pageno=3&fulltime=True&proximity=20'\n",
    "page3 = requests.get(URL3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup3 = BeautifulSoup(page3.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_list = [soup1,soup2,soup3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop acquiring job description, salary, location, type of contract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions_all = []\n",
    "salaries_all = []\n",
    "locations_all = []\n",
    "contract_all = []\n",
    "job_ref_all = []\n",
    "\n",
    "for i in range(len(pages_list)):\n",
    "    desc = pages_list[i].findAll(\"div\", {\"class\": \"description\"})\n",
    "    sal = pages_list[i].findAll(\"li\", {\"class\": \"salary\"})\n",
    "    loc = pages_list[i].findAll(\"li\", {\"class\": \"location\"})\n",
    "    con = pages_list[i].findAll(\"li\", {\"class\": \"time\"})\n",
    "    job_ref = pages_list[i].findAll(\"a\", {\"data-id\": True})\n",
    "    #job_id = job_refs[1][\"data-id\"]\n",
    "    if i !=0:\n",
    "        desc = desc[2:]\n",
    "        sal = sal[2:]\n",
    "        loc = loc[2:]\n",
    "        con = con[2:]\n",
    "        job_ref = job_ref[2:]\n",
    "        \n",
    "    descriptions_all.append(desc)\n",
    "    salaries_all.append(sal)\n",
    "    locations_all.append(loc)\n",
    "    contract_all.append(con)\n",
    "    job_ref_all.append(job_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_ads_container = {}\n",
    "\n",
    "for i in range(len(job_ref_all)):\n",
    "    for j in range(len(job_ref_all[i])):\n",
    "        job_id = job_ref_all[i][j][\"data-id\"]\n",
    "        job_dict = {}\n",
    "        \n",
    "        '''\n",
    "        Job page scraping, to get job description\n",
    "        '''\n",
    "        job_URL = 'https://www.reed.co.uk/jobs/data-scientist/'+job_id\n",
    "        job_page = requests.get(job_URL)\n",
    "        job_soup = BeautifulSoup(job_page.content, 'html.parser')\n",
    "        job_desc = job_soup.findAll(\"span\", {\"itemprop\": \"description\"})\n",
    "        job_desc=job_desc[0]\n",
    "        \n",
    "        ##################################\n",
    "        job_dict['Description'] = job_desc.text\n",
    "        try:\n",
    "            job_dict['Salary'] = salaries_all[i][j].contents[0]\n",
    "        except IndexError:\n",
    "            job_dict['Salary'] = None\n",
    "        try:\n",
    "            job_dict['Location'] = locations_all[i][j].contents[1].contents[0]\n",
    "        except IndexError:\n",
    "            job_dict['Location'] = None\n",
    "        try:\n",
    "            job_dict['Contract'] = contract_all[i][j].contents[0]\n",
    "        except IndexError:\n",
    "            job_dict['Contract'] = None\n",
    "        \n",
    "        job_ads_container[job_id] = job_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the job ads dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Save a dictionary into a pickle file.\n",
    "#import pickle\n",
    "import json\n",
    "\n",
    "with open('data/job_ads_container.json', 'w') as fp:\n",
    "    json.dump(job_ads_container, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON file\n",
    "with open('data/job_ads_container.json') as data_file:\n",
    "    job_ads_container_load = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(job_ads_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix - Scraping job page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_URL = 'https://www.reed.co.uk/jobs/senior-data-scientist/'+job_refs[1][\"data-id\"]\n",
    "job_page = requests.get(job_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_soup = BeautifulSoup(job_page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_desc = job_soup.findAll(\"span\", {\"itemprop\": \"description\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p>Alexa Shopping Spoken Language Understanding team is looking for a senior data scientist to join a recently established team in London. The team’s mission is to ‘put the customer in the loop’ to improve the shopping experience on Alexa. The team will be  building systems to collect and use explicit and implicit signals from customer behavior. As the lead scientist on the team you will drive the investigation to make this data actionable. You will have considerable scope to direct the research to find the maximum  impact. This could include identifying leading indicators of customer dissatisfaction, using customer provided signals to improve existing Alexa language understanding models or operationalizing entirely new models to improve the experience for our customers.<br/><br/>This is a blue-sky role that gives you a chance to roll up your sleeves and dive into big data sets in order to build simulations and experimentation systems at scale, build optimization algorithms and leverage cutting-edge technologies across Amazon. This  is an opportunity to think big about how to interpret the behavior of our customers and improve the voice shopping experience.<br/><br/>You will work closely with product and technical leaders throughout Alexa Shopping and will be responsible for influencing technical decisions in areas of development/modelling that you identify as critical future product offerings. You will identify both enablers  and blockers of adoption for product understanding, and build programs to raise the bar in terms of understanding product questions and predict the shaping of customer utterances as we move from simple to complex utterances.<br/><br/>The ideal candidate will have extensive experience in Science work, business analytics and have the aptitude to incorporate new approaches and methodologies while dealing with ambiguities in sourcing processes. Excellent business and communication skills are  a must to develop and define key business questions and to build data sets that answer those questions. You should have a demonstrated ability to think strategically and analytically about business, product, and technical challenges. Further, you must have  the ability to build and communicate compelling value propositions, and work across the organization to achieve consensus. This role requires a strong passion for customers, a high level of comfort navigating ambiguity, and a keen sense of ownership and drive  to deliver results.<br/><br/>Amazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation</p>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_desc[0].contents[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"description\">\n",
       "<p>We are currently recruiting a enthusiastic and bold Senior <span class=\"highlight\">Data</span> <span class=\"highlight\">Scientist</span> for a leading dating/social network organisation. As a Senior <span class=\"highlight\">Data</span> <span class=\"highlight\">Scientist</span> you will be responsible for working with large, complex <span class=\"highlight\">data</span> sets and much more. The Senior <span class=\"highlight\">Data</span> <span class=\"highlight\">Scientist</span>...</p>\n",
       "</div>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptions[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
